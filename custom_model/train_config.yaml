# ============================================================================
# Tricorder-3 Training Configuration
# ============================================================================
# This file configures the model training. Edit values below and run:
#   python custom_model/train.py
#
# Command-line arguments will override these settings.
# ============================================================================

# ----------------------------------------------------------------------------
# Data Settings
# ----------------------------------------------------------------------------
data:
  data_dir: "training_data"                    # Path to dataset directory
  labels_file: "training_data/labels_final.csv" # Path to labels CSV (null = auto-detect)
  val_split: 0.2                               # Validation split ratio (0.0-1.0)

# ----------------------------------------------------------------------------
# Model Settings
# ----------------------------------------------------------------------------
model:
  type: "balanced"                             # Options: lightweight, balanced, larger
  # lightweight: ~20MB, faster but less accurate
  # balanced:    ~45MB, good balance (recommended)
  # larger:      ~80MB, more accurate but slower

# ----------------------------------------------------------------------------
# Training Settings
# ----------------------------------------------------------------------------
training:
  epochs: 50                                   # Number of training epochs
  batch_size: 16                               # Batch size (reduce if out of memory)
  learning_rate: 0.001                         # Initial learning rate
  weight_decay: 0.01                           # L2 regularization
  dropout: 0.4                                 # Dropout rate
  patience: 10                                 # Early stopping patience
  num_workers: 4                               # Data loader workers (0 for Windows)
  seed: 42                                     # Random seed for reproducibility

# ----------------------------------------------------------------------------
# Data Augmentation
# ----------------------------------------------------------------------------
augmentation:
  use_mixup: true                              # Enable mixup augmentation
  mixup_alpha: 0.4                             # Mixup interpolation strength
  label_smoothing: 0.1                         # Label smoothing factor
  weighted_sampling: true                      # Oversample HIGH_RISK classes

# ----------------------------------------------------------------------------
# Checkpointing
# ----------------------------------------------------------------------------
checkpoints:
  save_dir: "checkpoints"                      # Directory to save checkpoints
  save_every: 5                                # Save every N epochs (0 = only best)
  export_onnx: true                            # Export to ONNX after training
  resume: null                                 # Path to checkpoint to resume from
                                               # Example: "checkpoints/latest.pt"

# ----------------------------------------------------------------------------
# Logging
# ----------------------------------------------------------------------------
logging:
  tensorboard: true                            # Enable TensorBoard logging
  tensorboard_dir: "runs"                      # TensorBoard log directory

# ----------------------------------------------------------------------------
# PyTorch Settings
# ----------------------------------------------------------------------------
pytorch:
  weights_only: false                          # Set false for PyTorch 2.6+ compatibility

# ----------------------------------------------------------------------------
# HuggingFace Settings
# ----------------------------------------------------------------------------
# Required for downloading competition datasets and some public datasets
# Get your token at: https://huggingface.co/settings/tokens
huggingface:
  token: your_hf_token                                  # Your HuggingFace token (or use HF_TOKEN env var)
                                               # Example: "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# ----------------------------------------------------------------------------
# Dataset Download Settings
# ----------------------------------------------------------------------------
# Used by: python custom_model/download_datasets.py
# 
# RECOMMENDED: Start with ISIC MILK10k - it has ALL 11 classes!
#   python custom_model/download_datasets.py --source isic
#
# To download ALL datasets: python custom_model/download_datasets.py --all
download:
  output_dir: "training_data"                  # Where to save downloaded datasets
  
  # ⭐ ISIC MILK10k - THE RECOMMENDED DATASET! ⭐
  # Has all 11 Tricorder-3 classes including BEN_OTH, INF, MAL_OTH!
  # Download from: https://challenge.isic-archive.com/landing/milk10k/
  isic:
    milk10k: true                              # 5,240 lesions - ALL 11 CLASSES!
  
  # Kaggle datasets (requires Kaggle API setup)
  # These can supplement MILK10k with more data for common classes
  kaggle:
    ham10000: true                             # 10,015 images, 7 classes
    isic_2019: true                            # 25,331 images, 8 classes
    isic_2020: false                           # 33,126 images, melanoma only
    skin_cancer_9class: false                  # 9-class extended dataset
    dermnet: false                             # 23,000 images, 23 classes
    isic_2018: false                           # ISIC 2018 challenge
    skin_lesion_7class: false                  # HAM10000 with segmentation
  
  # HuggingFace datasets (auto-download, no API needed)
  huggingface:
    marmal88_skin: false                       # Skin cancer from HuggingFace
    isic_hf: false                             # ISIC on HuggingFace
  
  # Direct downloads (URLs)
  direct:
    ph2: false                                 # 200 images, melanoma/nevi

# ----------------------------------------------------------------------------
# Dataset Split Settings
# ----------------------------------------------------------------------------
# Used by: python custom_model/split_dataset.py
split:
  train_ratio: 0.8                             # 80% for training
  val_ratio: 0.1                               # 10% for validation
  test_ratio: 0.1                              # 10% for testing (held out)
  stratify: true                               # Stratified split (recommended)
  balance: true                                # Balance training set
  max_per_class: 3000                          # Max samples per class when balancing
  min_per_class: 500                           # Min samples per class when balancing
