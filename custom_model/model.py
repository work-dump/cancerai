"""
Tricorder-3 Competition Model - Option A: Pretrained Backbone with Cross-Attention Fusion

Architecture:
- EfficientNet backbone (pretrained on ImageNet)
- Learned embeddings for demographics (age, sex, location)
- Cross-attention fusion between image and metadata features
- Multi-layer classifier with regularization

Input:
- image: (batch, 3, 512, 512) - normalized [0, 1]
- demographics: (batch, 3) - [age, sex, location]

Output:
- probabilities: (batch, 11) - softmax probabilities for 11 classes

Classes (in order):
0: AKIEC - Actinic keratosis (Medium risk)
1: BCC   - Basal cell carcinoma (Malignant)
2: BEN_OTH - Other benign (Benign)
3: BKL   - Benign keratosis-like (Medium risk)
4: DF    - Dermatofibroma (Benign)
5: INF   - Inflammatory (Benign)
6: MAL_OTH - Other malignant (Malignant)
7: MEL   - Melanoma (Malignant) - CRITICAL
8: NV    - Melanocytic nevus (Benign)
9: SCCKA - SCC/Keratoacanthoma (Malignant)
10: VASC - Vascular lesions (Medium risk)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict, Any
import math


# ============================================================================
# Constants
# ============================================================================

NUM_CLASSES = 11
IMAGE_SIZE = (512, 512)
DEMOGRAPHICS_DIM = 3  # age, sex, location

# Class information
CLASS_NAMES = [
    "AKIEC", "BCC", "BEN_OTH", "BKL", "DF", 
    "INF", "MAL_OTH", "MEL", "NV", "SCCKA", "VASC"
]

CLASS_WEIGHTS = {
    "MALIGNANT": [1, 6, 7, 9],      # BCC, MAL_OTH, MEL, SCCKA - weight 3x
    "MEDIUM_RISK": [0, 3, 10],       # AKIEC, BKL, VASC - weight 2x
    "BENIGN": [2, 4, 5, 8],          # BEN_OTH, DF, INF, NV - weight 1x
}

# Body location mapping
LOCATION_MAP = {
    1: "Arm",
    2: "Feet", 
    3: "Genitalia",
    4: "Hand",
    5: "Head",
    6: "Leg",
    7: "Torso",
}


# ============================================================================
# Backbone Options
# ============================================================================

BACKBONE_CONFIGS = {
    "efficientnet_b0": {"features": 1280, "size_mb": 20},
    "efficientnet_b1": {"features": 1280, "size_mb": 30},
    "efficientnet_b2": {"features": 1408, "size_mb": 35},
    "mobilenetv3_large": {"features": 960, "size_mb": 22},
    "convnext_tiny": {"features": 768, "size_mb": 110},
}


# ============================================================================
# Model Components
# ============================================================================

class DemographicsEncoder(nn.Module):
    """
    Encodes patient demographics (age, sex, location) using learned embeddings.
    
    This is more expressive than simple linear layers as it can learn
    distinct representations for categorical variables.
    """
    
    def __init__(
        self,
        output_dim: int = 256,
        age_embed_dim: int = 64,
        sex_embed_dim: int = 32,
        location_embed_dim: int = 64,
        dropout: float = 0.3,
    ):
        super().__init__()
        
        # Age encoder (continuous â†’ embedding)
        self.age_encoder = nn.Sequential(
            nn.Linear(1, 32),
            nn.GELU(),
            nn.Linear(32, age_embed_dim),
            nn.LayerNorm(age_embed_dim),
        )
        
        # Sex embedding (categorical: 0=female, 1=male, 2=unknown)
        self.sex_embedding = nn.Embedding(3, sex_embed_dim)
        
        # Location embedding (categorical: 1-7 locations + 0 for unknown)
        self.location_embedding = nn.Embedding(8, location_embed_dim)
        
        # Combine all demographics
        combined_dim = age_embed_dim + sex_embed_dim + location_embed_dim
        self.fusion = nn.Sequential(
            nn.Linear(combined_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(output_dim, output_dim),
            nn.LayerNorm(output_dim),
        )
        
        self.output_dim = output_dim
    
    def forward(self, demographics: torch.Tensor) -> torch.Tensor:
        """
        Args:
            demographics: (batch, 3) tensor with [age, sex, location]
                - age: float (years, e.g., 45.0)
                - sex: int (0=female, 1=male)
                - location: int (1-7)
        
        Returns:
            (batch, output_dim) demographic features
        """
        # Extract and process each component
        age = demographics[:, 0:1] / 100.0  # Normalize age to ~[0, 1]
        sex = demographics[:, 1].long().clamp(0, 2)
        location = demographics[:, 2].long().clamp(0, 7)
        
        # Get embeddings
        age_emb = self.age_encoder(age)
        sex_emb = self.sex_embedding(sex)
        location_emb = self.location_embedding(location)
        
        # Concatenate and fuse
        combined = torch.cat([age_emb, sex_emb, location_emb], dim=-1)
        return self.fusion(combined)


class CrossAttentionFusion(nn.Module):
    """
    Cross-attention module where image features attend to metadata features.
    
    This allows the model to learn which aspects of the image are relevant
    given the patient's demographic information.
    """
    
    def __init__(
        self,
        image_dim: int,
        meta_dim: int,
        num_heads: int = 4,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        # Project metadata to image dimension for attention
        self.meta_projection = nn.Linear(meta_dim, image_dim)
        
        # Multi-head cross-attention
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=image_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True,
        )
        
        # Layer norm for residual connection
        self.norm = nn.LayerNorm(image_dim)
        
        self.output_dim = image_dim * 2  # Original + attended
    
    def forward(
        self, 
        image_features: torch.Tensor, 
        meta_features: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            image_features: (batch, image_dim)
            meta_features: (batch, meta_dim)
        
        Returns:
            (batch, image_dim * 2) fused features
        """
        # Add sequence dimension for attention
        img_query = image_features.unsqueeze(1)  # (batch, 1, image_dim)
        meta_kv = self.meta_projection(meta_features).unsqueeze(1)  # (batch, 1, image_dim)
        
        # Cross-attention: image attends to metadata
        attended, _ = self.cross_attention(img_query, meta_kv, meta_kv)
        attended = attended.squeeze(1)  # (batch, image_dim)
        
        # Residual connection with normalization
        attended = self.norm(attended + image_features)
        
        # Concatenate original and attended features
        return torch.cat([image_features, attended], dim=-1)


class ClassifierHead(nn.Module):
    """
    Multi-layer classifier with regularization.
    
    Uses multiple FC layers with GELU activation, LayerNorm, and Dropout
    for better generalization.
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_dims: Tuple[int, ...] = (512, 256),
        num_classes: int = NUM_CLASSES,
        dropout: float = 0.4,
    ):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout),
            ])
            prev_dim = hidden_dim
        
        # Final classification layer (no activation - softmax applied in forward)
        layers.append(nn.Linear(prev_dim, num_classes))
        
        self.classifier = nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, input_dim) fused features
        
        Returns:
            (batch, num_classes) logits
        """
        return self.classifier(x)


# ============================================================================
# Main Model
# ============================================================================

class Tricorder3Model(nn.Module):
    """
    Tricorder-3 Competition Model with Pretrained Backbone and Cross-Attention Fusion.
    
    Architecture:
        1. Image Encoder: EfficientNet (pretrained, optionally frozen)
        2. Demographics Encoder: Learned embeddings + MLP
        3. Fusion: Cross-attention between image and demographics
        4. Classifier: Multi-layer FC with regularization
    
    Example:
        >>> model = Tricorder3Model(backbone="efficientnet_b1", freeze_backbone=True)
        >>> image = torch.randn(4, 3, 512, 512)
        >>> demographics = torch.tensor([[45.0, 1.0, 5.0], ...])  # age, sex, location
        >>> probs = model(image, demographics)
        >>> print(probs.shape)  # (4, 11)
    """
    
    def __init__(
        self,
        backbone: str = "efficientnet_b1",
        freeze_backbone: bool = True,
        pretrained: bool = True,
        meta_dim: int = 256,
        num_attention_heads: int = 4,
        classifier_hidden_dims: Tuple[int, ...] = (512, 256),
        dropout: float = 0.4,
    ):
        """
        Args:
            backbone: Backbone architecture name (see BACKBONE_CONFIGS)
            freeze_backbone: Whether to freeze backbone weights (recommended for small datasets)
            pretrained: Use ImageNet pretrained weights
            meta_dim: Dimension of demographics embedding
            num_attention_heads: Number of heads in cross-attention
            classifier_hidden_dims: Hidden layer dimensions for classifier
            dropout: Dropout probability
        """
        super().__init__()
        
        self.backbone_name = backbone
        self.freeze_backbone = freeze_backbone
        
        # ============ Image Encoder ============
        self.backbone = self._create_backbone(backbone, pretrained)
        self.image_dim = self._get_backbone_features(backbone)
        
        if freeze_backbone:
            self._freeze_backbone()
        
        # ============ Demographics Encoder ============
        self.demographics_encoder = DemographicsEncoder(
            output_dim=meta_dim,
            dropout=dropout,
        )
        
        # ============ Cross-Attention Fusion ============
        self.fusion = CrossAttentionFusion(
            image_dim=self.image_dim,
            meta_dim=meta_dim,
            num_heads=num_attention_heads,
            dropout=dropout,
        )
        
        # ============ Classifier ============
        self.classifier = ClassifierHead(
            input_dim=self.fusion.output_dim,
            hidden_dims=classifier_hidden_dims,
            num_classes=NUM_CLASSES,
            dropout=dropout,
        )
        
        # Initialize weights
        self._init_weights()
    
    def _create_backbone(self, backbone: str, pretrained: bool) -> nn.Module:
        """Create backbone network using timm."""
        try:
            import timm
        except ImportError:
            raise ImportError("Please install timm: pip install timm")
        
        model = timm.create_model(
            backbone,
            pretrained=pretrained,
            num_classes=0,  # Remove classification head
            global_pool="avg",
        )
        return model
    
    def _get_backbone_features(self, backbone: str) -> int:
        """Get output feature dimension for backbone."""
        if backbone in BACKBONE_CONFIGS:
            return BACKBONE_CONFIGS[backbone]["features"]
        
        # Fallback: run a forward pass to determine
        with torch.no_grad():
            dummy = torch.randn(1, 3, 512, 512)
            features = self.backbone(dummy)
            return features.shape[-1]
    
    def _freeze_backbone(self):
        """Freeze backbone parameters."""
        for param in self.backbone.parameters():
            param.requires_grad = False
    
    def _init_weights(self):
        """Initialize classifier weights."""
        for module in [self.demographics_encoder, self.fusion, self.classifier]:
            for m in module.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif isinstance(m, nn.Embedding):
                    nn.init.normal_(m.weight, mean=0, std=0.02)
    
    def unfreeze_backbone(self, unfreeze_layers: Optional[int] = None):
        """
        Unfreeze backbone for fine-tuning.
        
        Args:
            unfreeze_layers: Number of layers to unfreeze from the end.
                            If None, unfreeze all layers.
        """
        if unfreeze_layers is None:
            for param in self.backbone.parameters():
                param.requires_grad = True
        else:
            # Get all parameters as a list
            params = list(self.backbone.parameters())
            # Unfreeze last N parameters
            for param in params[-unfreeze_layers:]:
                param.requires_grad = True
        
        self.freeze_backbone = False
    
    def forward(
        self, 
        image: torch.Tensor, 
        demographics: torch.Tensor
    ) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            image: (batch, 3, 512, 512) - RGB image normalized to [0, 1]
            demographics: (batch, 3) - [age, sex, location]
                - age: float (years)
                - sex: 0=female, 1=male
                - location: 1-7 (body location)
        
        Returns:
            (batch, 11) - class probabilities (softmax)
        """
        # Extract image features
        if self.freeze_backbone:
            with torch.no_grad():
                image_features = self.backbone(image)
        else:
            image_features = self.backbone(image)
        
        # Encode demographics
        meta_features = self.demographics_encoder(demographics)
        
        # Fuse modalities with cross-attention
        fused_features = self.fusion(image_features, meta_features)
        
        # Classify
        logits = self.classifier(fused_features)
        
        # Apply softmax for probabilities
        probabilities = F.softmax(logits, dim=-1)
        
        return probabilities
    
    def forward_with_logits(
        self,
        image: torch.Tensor,
        demographics: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass returning both logits and probabilities.
        Useful for training with CrossEntropyLoss.
        
        Returns:
            (logits, probabilities)
        """
        if self.freeze_backbone:
            with torch.no_grad():
                image_features = self.backbone(image)
        else:
            image_features = self.backbone(image)
        
        meta_features = self.demographics_encoder(demographics)
        fused_features = self.fusion(image_features, meta_features)
        logits = self.classifier(fused_features)
        probabilities = F.softmax(logits, dim=-1)
        
        return logits, probabilities
    
    def get_model_size_mb(self) -> float:
        """Get model size in megabytes."""
        param_size = sum(p.numel() * p.element_size() for p in self.parameters())
        buffer_size = sum(b.numel() * b.element_size() for b in self.buffers())
        return (param_size + buffer_size) / (1024 * 1024)
    
    def count_parameters(self) -> Dict[str, int]:
        """Count trainable and total parameters."""
        total = sum(p.numel() for p in self.parameters())
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        return {
            "total": total,
            "trainable": trainable,
            "frozen": total - trainable,
        }


# ============================================================================
# Export Functions
# ============================================================================

def export_to_onnx(
    model: Tricorder3Model,
    output_path: str,
    batch_size: int = 1,
    opset_version: int = 14,
) -> str:
    """
    Export model to ONNX format for submission.
    
    Args:
        model: Trained Tricorder3Model
        output_path: Path to save ONNX model
        batch_size: Batch size for export (use 1 for dynamic)
        opset_version: ONNX opset version
    
    Returns:
        Path to saved ONNX model
    """
    # Move model to CPU for ONNX export (required by PyTorch ONNX exporter)
    model = model.cpu()
    model.eval()
    
    # Create dummy inputs on CPU
    dummy_image = torch.randn(batch_size, 3, 512, 512, device='cpu')
    dummy_demographics = torch.tensor([[45.0, 1.0, 5.0]] * batch_size, device='cpu')
    
    # Export using legacy exporter (more reliable than dynamo-based)
    torch.onnx.export(
        model,
        (dummy_image, dummy_demographics),
        output_path,
        input_names=["image", "demographics"],
        output_names=["output"],
        dynamic_axes={
            "image": {0: "batch_size"},
            "demographics": {0: "batch_size"},
            "output": {0: "batch_size"},
        },
        opset_version=opset_version,
        do_constant_folding=True,
        dynamo=False,  # Use legacy exporter for better compatibility
    )
    
    print(f"Model exported to: {output_path}")
    return output_path


def verify_onnx_model(onnx_path: str) -> bool:
    """Verify ONNX model is valid and test inference."""
    try:
        import onnx
        import onnxruntime as ort
        import numpy as np
        
        # Load and check model
        model = onnx.load(onnx_path)
        onnx.checker.check_model(model)
        print("ONNX model structure is valid")
        
        # Test inference
        session = ort.InferenceSession(onnx_path)
        
        dummy_image = np.random.rand(1, 3, 512, 512).astype(np.float32)
        dummy_demo = np.array([[45.0, 1.0, 5.0]], dtype=np.float32)
        
        outputs = session.run(
            None, 
            {"image": dummy_image, "demographics": dummy_demo}
        )
        
        probs = outputs[0]
        print(f"Output shape: {probs.shape}")
        print(f"Output sum: {probs.sum():.4f} (should be ~1.0)")
        print(f"Output range: [{probs.min():.4f}, {probs.max():.4f}]")
        
        # Verify softmax
        if abs(probs.sum() - 1.0) < 0.01:
            print("Softmax verification: PASSED")
            return True
        else:
            print("Softmax verification: FAILED")
            return False
            
    except Exception as e:
        print(f"Verification failed: {e}")
        return False


# ============================================================================
# Factory Functions
# ============================================================================

def create_model(
    backbone: str = "efficientnet_b1",
    freeze_backbone: bool = True,
    **kwargs
) -> Tricorder3Model:
    """
    Create a Tricorder3Model with specified configuration.
    
    Args:
        backbone: Backbone name (efficientnet_b0, efficientnet_b1, efficientnet_b2, etc.)
        freeze_backbone: Whether to freeze backbone (recommended for fine-tuning)
        **kwargs: Additional arguments passed to Tricorder3Model
    
    Returns:
        Initialized Tricorder3Model
    """
    return Tricorder3Model(
        backbone=backbone,
        freeze_backbone=freeze_backbone,
        **kwargs
    )


def create_lightweight_model() -> Tricorder3Model:
    """Create a lightweight model (<30MB) for maximum efficiency score."""
    return Tricorder3Model(
        backbone="efficientnet_b0",
        freeze_backbone=True,
        meta_dim=128,
        classifier_hidden_dims=(256, 128),
        dropout=0.3,
    )


def create_balanced_model() -> Tricorder3Model:
    """
    Create a balanced model (<50MB) with good accuracy/efficiency tradeoff.
    Uses EfficientNet-B0 backbone with larger classifier for full efficiency score.
    """
    return Tricorder3Model(
        backbone="efficientnet_b0",
        freeze_backbone=True,
        meta_dim=256,
        num_attention_heads=4,
        classifier_hidden_dims=(512, 256),
        dropout=0.4,
    )


def create_larger_model() -> Tricorder3Model:
    """
    Create a larger model (~57MB) with EfficientNet-B1.
    Slightly lower efficiency score (0.93) but potentially higher accuracy.
    """
    return Tricorder3Model(
        backbone="efficientnet_b1",
        freeze_backbone=True,
        meta_dim=256,
        num_attention_heads=4,
        classifier_hidden_dims=(512, 256),
        dropout=0.4,
    )


def create_accurate_model() -> Tricorder3Model:
    """
    Create a model optimized for accuracy (~80MB) with fine-tuning enabled.
    Lower efficiency score (0.70) but maximum accuracy potential.
    Use this if you have lots of training data.
    """
    return Tricorder3Model(
        backbone="efficientnet_b2",
        freeze_backbone=False,  # Fine-tune backbone
        meta_dim=256,
        num_attention_heads=8,
        classifier_hidden_dims=(512, 256),
        dropout=0.3,
    )


# ============================================================================
# Advanced Model Architecture (Based on ISIC Competition Winners)
# ============================================================================

class GeM(nn.Module):
    """
    Generalized Mean Pooling.
    
    Better than average pooling for image classification.
    Used in many ISIC competition winning solutions.
    """
    def __init__(self, p: float = 3.0, eps: float = 1e-6):
        super().__init__()
        self.p = nn.Parameter(torch.ones(1) * p)
        self.eps = eps
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, channels, H, W)
        return x.clamp(min=self.eps).pow(self.p).mean(dim=[-2, -1]).pow(1.0 / self.p)


class SEBlock(nn.Module):
    """
    Squeeze-and-Excitation block for feature recalibration.
    """
    def __init__(self, channels: int, reduction: int = 16):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool1d(1)
        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, channels)
        scale = self.excitation(x)
        return x * scale


class MetadataGating(nn.Module):
    """
    Gated fusion of image features with metadata.
    
    Uses learned gating to control how much metadata influences predictions.
    This is more effective than simple concatenation.
    """
    def __init__(self, image_dim: int, meta_dim: int, hidden_dim: int = 256):
        super().__init__()
        
        # Metadata projection
        self.meta_proj = nn.Sequential(
            nn.Linear(meta_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, image_dim),
        )
        
        # Gating mechanism
        self.gate = nn.Sequential(
            nn.Linear(image_dim * 2, image_dim),
            nn.Sigmoid()
        )
        
        # Output projection
        self.output_proj = nn.Sequential(
            nn.Linear(image_dim, image_dim),
            nn.LayerNorm(image_dim),
            nn.GELU(),
        )
        
    def forward(self, image_features: torch.Tensor, meta_features: torch.Tensor) -> torch.Tensor:
        # Project metadata to image feature space
        meta_proj = self.meta_proj(meta_features)
        
        # Compute gate
        combined = torch.cat([image_features, meta_proj], dim=-1)
        gate = self.gate(combined)
        
        # Gated fusion
        fused = image_features + gate * meta_proj
        
        return self.output_proj(fused)


class ISICWinnerModel(nn.Module):
    """
    Advanced model architecture based on ISIC competition winning solutions.
    
    Key improvements over base Tricorder3Model:
    1. GeM pooling instead of average pooling (better for classification)
    2. SE blocks for feature recalibration
    3. Gated metadata fusion (more effective than cross-attention for this task)
    4. Larger EfficientNet backbone (B4) with partial fine-tuning
    5. Multi-scale feature aggregation
    6. Label smoothing built into loss
    
    Reference: Top solutions from ISIC 2019, 2020, 2024 challenges.
    """
    
    def __init__(
        self,
        backbone: str = "efficientnet_b4",
        pretrained: bool = True,
        freeze_ratio: float = 0.7,  # Freeze first 70% of backbone layers
        meta_dim: int = 128,
        classifier_dims: Tuple[int, ...] = (512, 256),
        dropout: float = 0.5,
        use_gem: bool = True,
    ):
        super().__init__()
        
        self.backbone_name = backbone
        
        # ============ Image Encoder ============
        try:
            import timm
        except ImportError:
            raise ImportError("Please install timm: pip install timm")
        
        # Create backbone
        self.backbone = timm.create_model(
            backbone,
            pretrained=pretrained,
            num_classes=0,  # Remove classifier
            global_pool='',  # Remove global pooling (we use GeM)
        )
        
        # Check if model requires specific input size (e.g., ViT models)
        self.backbone_input_size = None
        if hasattr(self.backbone, 'default_cfg'):
            cfg = self.backbone.default_cfg
            input_size = cfg.get('input_size', (3, 224, 224))
            if input_size[1] != 512 or input_size[2] != 512:
                self.backbone_input_size = (input_size[1], input_size[2])
                print(f"  Note: {backbone} expects {input_size[1]}x{input_size[2]} input, will resize from 512x512")
        
        # Get feature dimensions (use correct input size)
        test_size = self.backbone_input_size if self.backbone_input_size else (512, 512)
        with torch.no_grad():
            dummy = torch.randn(1, 3, test_size[0], test_size[1])
            features = self.backbone(dummy)
            if len(features.shape) == 4:
                self.image_dim = features.shape[1]
            else:
                self.image_dim = features.shape[-1]
        
        # Partial freeze (freeze first N% of layers)
        if freeze_ratio > 0:
            self._partial_freeze(freeze_ratio)
        
        # GeM pooling
        self.use_gem = use_gem
        if use_gem:
            self.pool = GeM(p=3.0)
        else:
            self.pool = nn.AdaptiveAvgPool2d(1)
        
        # SE block for image features
        self.se_block = SEBlock(self.image_dim, reduction=16)
        
        # ============ Metadata Encoder ============
        self.meta_encoder = nn.Sequential(
            nn.Linear(DEMOGRAPHICS_DIM, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(64, meta_dim),
            nn.LayerNorm(meta_dim),
            nn.GELU(),
        )
        
        # ============ Gated Fusion ============
        self.fusion = MetadataGating(
            image_dim=self.image_dim,
            meta_dim=meta_dim,
            hidden_dim=256,
        )
        
        # ============ Classifier ============
        layers = []
        prev_dim = self.image_dim
        
        for hidden_dim in classifier_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),  # BatchNorm often works better than LayerNorm
                nn.GELU(),
                nn.Dropout(dropout),
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, NUM_CLASSES))
        self.classifier = nn.Sequential(*layers)
        
        # Initialize classifier weights
        self._init_classifier()
    
    def _partial_freeze(self, ratio: float):
        """Freeze first ratio% of backbone parameters."""
        params = list(self.backbone.parameters())
        freeze_until = int(len(params) * ratio)
        
        for i, param in enumerate(params):
            if i < freeze_until:
                param.requires_grad = False
    
    def _init_classifier(self):
        """Initialize classifier with small weights."""
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def _extract_features(self, image: torch.Tensor) -> torch.Tensor:
        """Extract features from image, handling input size differences."""
        # Resize if backbone requires different input size (e.g., ViT models)
        if self.backbone_input_size is not None:
            image = F.interpolate(
                image, 
                size=self.backbone_input_size, 
                mode='bilinear', 
                align_corners=False
            )
        
        # Extract features
        features = self.backbone(image)
        
        # Handle different output formats:
        # - CNN (EfficientNet): (B, C, H, W) -> need to pool
        # - ViT/DeiT: (B, num_patches+1, features) -> take [CLS] token or mean pool
        # - Swin: (B, H, W, C) -> need to permute and pool
        
        if len(features.shape) == 4:
            # CNN output: (B, C, H, W)
            if features.shape[1] < features.shape[2]:
                # Swin-like: (B, H, W, C) - permute to (B, C, H, W)
                features = features.permute(0, 3, 1, 2)
            
            if self.use_gem:
                features = self.pool(features)
            else:
                features = self.pool(features).flatten(1)
                
        elif len(features.shape) == 3:
            # ViT/DeiT output: (B, num_patches+1, features)
            # Option 1: Use [CLS] token (index 0)
            # Option 2: Mean pool over all patches
            # Using [CLS] token as it's designed for classification
            features = features[:, 0]  # (B, features)
        
        # SE recalibration
        features = self.se_block(features)
        
        return features
    
    def forward(
        self,
        image: torch.Tensor,
        demographics: torch.Tensor
    ) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            image: (batch, 3, H, W) normalized image (512x512)
            demographics: (batch, 3) [age, sex, location]
        
        Returns:
            (batch, 11) class probabilities
        """
        # Extract image features
        features = self._extract_features(image)
        
        # Encode metadata
        meta_features = self.meta_encoder(demographics)
        
        # Gated fusion
        fused = self.fusion(features, meta_features)
        
        # Classify
        logits = self.classifier(fused)
        
        return F.softmax(logits, dim=-1)
    
    def forward_with_logits(
        self,
        image: torch.Tensor,
        demographics: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass returning both logits and probabilities."""
        # Extract image features
        features = self._extract_features(image)
        
        # Encode metadata
        meta_features = self.meta_encoder(demographics)
        
        # Gated fusion
        fused = self.fusion(features, meta_features)
        
        # Classify
        logits = self.classifier(fused)
        probs = F.softmax(logits, dim=-1)
        
        return logits, probs
    
    def count_parameters(self) -> Dict[str, int]:
        """Count model parameters."""
        total = sum(p.numel() for p in self.parameters())
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        return {
            "total": total,
            "trainable": trainable,
            "frozen": total - trainable,
        }
    
    def get_model_size_mb(self) -> float:
        """Estimate model size in MB (for ONNX export)."""
        total_params = sum(p.numel() for p in self.parameters())
        return (total_params * 4) / (1024 * 1024)


def create_isic_winner_model() -> ISICWinnerModel:
    """
    Create ISIC competition winner-style model.
    
    Uses EfficientNet-B4 with partial fine-tuning, GeM pooling,
    SE blocks, and gated metadata fusion.
    
    Model size: ~75MB (efficiency score ~0.75)
    Expected accuracy: Higher than base models
    """
    return ISICWinnerModel(
        backbone="efficientnet_b4",
        pretrained=True,
        freeze_ratio=0.5,  # Fine-tune last 50% of backbone
        meta_dim=128,
        classifier_dims=(512, 256),
        dropout=0.5,
        use_gem=True,
    )


def create_isic_winner_small() -> ISICWinnerModel:
    """
    Smaller ISIC winner-style model that fits under 50MB.
    
    Uses EfficientNet-B0 with advanced techniques for efficiency.
    
    Model size: ~45MB (efficiency score ~1.0)
    """
    return ISICWinnerModel(
        backbone="efficientnet_b0",
        pretrained=True,
        freeze_ratio=0.5,  # Fine-tune last 50% of backbone
        meta_dim=64,
        classifier_dims=(256, 128),
        dropout=0.4,
        use_gem=True,
    )


def create_convnext_model() -> ISICWinnerModel:
    """
    ConvNeXt-based model.
    
    ConvNeXt is a modernized CNN architecture that often
    outperforms Vision Transformers on image classification.
    
    Model size: ~110MB (efficiency score ~0.5)
    """
    return ISICWinnerModel(
        backbone="convnext_tiny",
        pretrained=True,
        freeze_ratio=0.5,
        meta_dim=128,
        classifier_dims=(512, 256),
        dropout=0.5,
        use_gem=False,  # ConvNeXt already has good pooling
    )


# ============================================================================
# Vision Transformer Models
# ============================================================================

def create_vit_model() -> ISICWinnerModel:
    """
    Vision Transformer (ViT) model.
    
    ViT-Small with partial fine-tuning.
    Good balance between accuracy and size.
    
    Model size: ~90MB (efficiency score ~0.6)
    """
    return ISICWinnerModel(
        backbone="vit_small_patch16_224",
        pretrained=True,
        freeze_ratio=0.6,  # Fine-tune last 40%
        meta_dim=128,
        classifier_dims=(512, 256),
        dropout=0.5,
        use_gem=False,  # ViT outputs (B, features) already
    )


def create_vit_small() -> ISICWinnerModel:
    """
    Smaller Vision Transformer that fits under 50MB.
    
    Uses ViT-Tiny for efficiency.
    
    Model size: ~35MB (efficiency score ~1.0)
    """
    return ISICWinnerModel(
        backbone="vit_tiny_patch16_224",
        pretrained=True,
        freeze_ratio=0.5,
        meta_dim=64,
        classifier_dims=(256, 128),
        dropout=0.4,
        use_gem=False,
    )


def create_deit_model() -> ISICWinnerModel:
    """
    DeiT (Data-efficient Image Transformer) model.
    
    DeiT is trained with better data augmentation and
    often performs better than vanilla ViT.
    
    Model size: ~90MB (efficiency score ~0.6)
    """
    return ISICWinnerModel(
        backbone="deit_small_patch16_224",
        pretrained=True,
        freeze_ratio=0.6,
        meta_dim=128,
        classifier_dims=(512, 256),
        dropout=0.5,
        use_gem=False,
    )


def create_deit_small() -> ISICWinnerModel:
    """
    Smaller DeiT that fits under 50MB.
    
    Model size: ~35MB (efficiency score ~1.0)
    """
    return ISICWinnerModel(
        backbone="deit_tiny_patch16_224",
        pretrained=True,
        freeze_ratio=0.5,
        meta_dim=64,
        classifier_dims=(256, 128),
        dropout=0.4,
        use_gem=False,
    )


def create_swin_model() -> ISICWinnerModel:
    """
    Swin Transformer model.
    
    Swin Transformer uses shifted windows for better efficiency
    and often achieves SOTA results on image classification.
    Popular in ISIC competition winning solutions.
    
    Model size: ~115MB (efficiency score ~0.5)
    """
    return ISICWinnerModel(
        backbone="swin_tiny_patch4_window7_224",
        pretrained=True,
        freeze_ratio=0.5,
        meta_dim=128,
        classifier_dims=(512, 256),
        dropout=0.5,
        use_gem=False,
    )


# ============================================================================
# Main (for testing)
# ============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("Tricorder-3 Model Architecture Test")
    print("=" * 60)
    
    # Create model
    print("\nCreating balanced model...")
    model = create_balanced_model()
    
    # Print model info
    params = model.count_parameters()
    print(f"\nModel Parameters:")
    print(f"  Total:     {params['total']:,}")
    print(f"  Trainable: {params['trainable']:,}")
    print(f"  Frozen:    {params['frozen']:,}")
    print(f"\nEstimated Size: {model.get_model_size_mb():.2f} MB")
    
    # Test forward pass
    print("\nTesting forward pass...")
    model.eval()
    
    batch_size = 2
    dummy_image = torch.randn(batch_size, 3, 512, 512)
    dummy_demographics = torch.tensor([
        [45.0, 1.0, 5.0],  # 45yo male, head
        [30.0, 0.0, 7.0],  # 30yo female, torso
    ])
    
    with torch.no_grad():
        probs = model(dummy_image, dummy_demographics)
    
    print(f"Input image shape: {dummy_image.shape}")
    print(f"Input demographics shape: {dummy_demographics.shape}")
    print(f"Output shape: {probs.shape}")
    print(f"Output sum (should be 1.0): {probs[0].sum():.4f}")
    print(f"Predictions:\n{probs}")
    
    # Test ONNX export
    print("\n" + "=" * 60)
    print("Testing ONNX Export")
    print("=" * 60)
    
    onnx_path = "/tmp/test_tricorder3_model.onnx"
    export_to_onnx(model, onnx_path)
    verify_onnx_model(onnx_path)
    
    print("\nAll tests passed!")
